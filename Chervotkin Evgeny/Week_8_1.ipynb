{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Пришел новый датасет. Его надо изучить."
      ],
      "metadata": {
        "id": "UCP6cs6LWGxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем датасет. Я скачал его на компьютер и добавил через загрузку файла в папку с датасетами. Пробуем присвоить переменную и открыть один из датасетов.\n"
      ],
      "metadata": {
        "id": "sxpaqjlkWHiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка модуля pandas\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7CMd8UGQYAKy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHtA-thnXACb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2485422-7b78-47ea-b9f5-65104af6c68d"
      },
      "source": [
        "# Загрузка файла из облака в colab\n",
        "import gdown\n",
        "gdown.download('https://storage.yandexcloud.net/terratraineeship/23_InspectrumClinic_RS/datasets/26.06%20new%20dataset.zip', None, quiet=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'26.06%20new%20dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Распакуем зипархив."
      ],
      "metadata": {
        "id": "vt7T8O5v7rcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Указываем путь к скачанному зип-архиву\n",
        "zip_path = '/content/26.06%20new%20dataset.zip'\n",
        "\n",
        "# Указываем путь для распаковки архива\n",
        "output_path = '/content/sample_data/'\n",
        "\n",
        "# Открываем зип-архив\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    # Распаковываем все файлы из архива в указанную папку\n",
        "    zip_ref.extractall(output_path)\n",
        "\n",
        "print(\"Архив успешно распакован.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sbCL7LO7zXs",
        "outputId": "a7aeeeee-7ed5-4ee0-a19c-b45fc7addcdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Архив успешно распакован.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Присвоение переменной\n",
        "data = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_01-22.csv')\n"
      ],
      "metadata": {
        "id": "DE167ILOiFf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "00f8af6c-e686-4185-e5fa-ba5ef5bc4ec3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-a9824b9ead79>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Присвоение переменной\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/26.06 new dataset/neiro_01-22.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 73, saw 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Появилась ошибка Error tokenizing data. C error: Expected 1 fields in line 73, saw 5 , повествующая нам, что при попытке разделить данные в файле CSV на поля, произошла ошибка. Конкретно в строке 73 файла было обнаружено больше полей, чем ожидалось.\n",
        "Значит попробуем открыть блокнотом на компудакторе. Посмотрим что там не так."
      ],
      "metadata": {
        "id": "tpwRwYUSWHk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ошибка возникла, потому что строка содержит символы, которые могут быть распознаны как разделители полей, хотя они на самом деле являются частью данных. В данном случае, это символы точки с запятой (;).** Чтобы правильно обработать эту строку, следует указать правильные параметры при чтении файла CSV. Воспользуйтесь параметром delimiter (разделитель) и quotechar (символ кавычек), чтобы указать правильные символы разделителя и экранирования."
      ],
      "metadata": {
        "id": "-Yhvqo2GbxyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_01-22.csv', delimiter=';', quotechar='\"')\n"
      ],
      "metadata": {
        "id": "OPPWNb3ccP_E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Убираем ограничение на вывод колонок с 20 до 400import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_columns', 400)\n"
      ],
      "metadata": {
        "id": "YnFWCsD6d0_B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим пекрвые 5 строк для анализа.\n",
        "data"
      ],
      "metadata": {
        "id": "80ZqssEOcZ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сработал метод. Открылся файл. Сразу вижу, что тут 1511 строк и 342 колонки, что несколько отличается от первого датасета в котором было 4500 колонок. а значит обрабатывать таблицу придется снова. Нужно будет удалить лишнее и разобраться почему таблицы разные и еще желательно сопоставить все таблицы датасета.**Отсутствуют названия колонок!** Датасетов 17 штук. Датасеты проименованы месяцем и годом, когда проходил медосмотр. Датасеты свободно открываются EXCEL, блокнотом. В датасете **neiro_06-23** есть названия колонок, в остальных нет, могу предположить, что названия совпадают со всеми данными в остальных датасетах, но нужно проверить."
      ],
      "metadata": {
        "id": "HdHyu8Rkck9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Есть более простой способ открытия и анализа датасетов. Заключается в пропуске проблемных строк. С помощью него присвоим каждому датасету новую переменную.**"
      ],
      "metadata": {
        "id": "C9wI2UjkWHnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_01_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_01-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_02_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_02-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_03_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_03-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_04_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_04-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_05_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_05-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_06_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_06-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_07_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_07-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_08_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro-08-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_09_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro-09-22 (1).csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_10_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro-10-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_12_22 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_12-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_01_23 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_01_23.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_02_23 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_02_23.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_03_23 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_03_23.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_04_23 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_04_23.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_05_23 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_05-22.csv', sep = ';', on_bad_lines='skip', engine='python' )\n",
        "data_06_23 = pd.read_csv('/content/sample_data/26.06 new dataset/neiro_06-23.csv', sep = ';', on_bad_lines='skip', engine='python' )"
      ],
      "metadata": {
        "id": "h3JiWuwEaKPL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Названия датасетов**\n",
        "data_01_22,\n",
        "data_02_22,\n",
        "data_03_22,\n",
        "data_04_22,\n",
        "data_05_22,\n",
        "data_06_22,\n",
        "data_07_22,\n",
        "data_08_22,\n",
        "data_09_22,\n",
        "data_10_22,\n",
        "data_12_22,\n",
        "data_01_23,\n",
        "data_02_23,\n",
        "data_03_23,\n",
        "data_04_23,\n",
        "data_05_23,\n",
        "data_06_23,"
      ],
      "metadata": {
        "id": "nBUSp9WNWHtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datasets = [data_01_22, data_02_22, data_03_22, data_04_22, data_05_22, data_06_22,\n",
        "            data_07_22, data_08_22, data_09_22, data_10_22, data_12_22, data_01_23,\n",
        "            data_02_23, data_03_23, data_04_23, data_05_23, data_06_23]\n",
        "\n",
        "for i, dataset in enumerate(datasets, start=1):\n",
        "    num_columns = dataset.shape[1]\n",
        "    print(f\"Датасет {i}: Количество колонок: {num_columns}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yYPm_-xe_z1",
        "outputId": "bf3749e9-664d-4419-a4f5-b08f550dcbb9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Датасет 1: Количество колонок: 342\n",
            "Датасет 2: Количество колонок: 294\n",
            "Датасет 3: Количество колонок: 446\n",
            "Датасет 4: Количество колонок: 306\n",
            "Датасет 5: Количество колонок: 262\n",
            "Датасет 6: Количество колонок: 458\n",
            "Датасет 7: Количество колонок: 218\n",
            "Датасет 8: Количество колонок: 304\n",
            "Датасет 9: Количество колонок: 310\n",
            "Датасет 10: Количество колонок: 258\n",
            "Датасет 11: Количество колонок: 222\n",
            "Датасет 12: Количество колонок: 228\n",
            "Датасет 13: Количество колонок: 290\n",
            "Датасет 14: Количество колонок: 430\n",
            "Датасет 15: Количество колонок: 266\n",
            "Датасет 16: Количество колонок: 262\n",
            "Датасет 17: Количество колонок: 190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Везде количество колонок разное. В 17 датасете data_06_23 есть названия колонок и их количество 190. Имеет смысл проанализировать его сначала, а потом склить с остальными датасетами, предварительно назвав колонки как надо."
      ],
      "metadata": {
        "id": "3ft6Zo0RWHwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gWWm6RfbWHyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RxpGwFMBWH1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o1k6kH62WH35"
      }
    }
  ]
}